import numpy as np
import pandas as pd
import polars as pl
import json
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score, mean_squared_error
from gensim.models import Word2Vec
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import string

# Load json file
with open("messages-000001.json", "rb") as f:
    data = json.load(f)

all_messages = []
all_senders = []

# Loop through each message in the 'zerver_message' column and extract the message content
for message in data["zerver_message"]:
    content = message["content"]
    sender = message["sender"]
    all_messages.append(content)
    all_senders.append(sender)

# Create a dictionary with column names and values
data = {"content": all_messages, "sender_id": all_senders}

# Create a Pandas DataFrame
df = pd.DataFrame(data)

# Convert the Pandas DataFrame to a Polars DataFrame
pl_df = pl.from_pandas(df)

# Text Preprocessing
def preprocess_text(text):
    # Remove punctuation
    text = text.translate(str.maketrans("", "", string.punctuation))
    # Convert to lowercase
    text = text.lower()
    return text

pl_df = pl_df.select(
    pl.col("content").apply(lambda x: len(x)).alias("Message Length"),
    pl.col("sender_id").alias("Sender ID")
)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    pl_df["Message Length"], pl_df["Sender ID"], test_size=0.2, random_state=42
)

# Perform Logistic Regression
logreg = LogisticRegression()
logreg.fit(X_train.to_numpy().reshape(-1, 1), y_train)
predictions_logreg = logreg.predict(X_test.to_numpy().reshape(-1, 1))
accuracy_logreg = accuracy_score(y_test, predictions_logreg)

# Perform Linear Regression
linreg = LinearRegression()
linreg.fit(X_train.to_numpy().reshape(-1, 1), y_train)
predictions_linreg = linreg.predict(X_test.to_numpy().reshape(-1, 1))
mse_linreg = mean_squared_error(y_test, predictions_linreg)

# Visualize Logistic Regression
plt.figure(figsize=(8, 6))
plt.scatter(X_test, y_test, c=predictions_logreg)
plt.title("Logistic Regression")
plt.xlabel("Message Length")
plt.ylabel("Sender ID")
plt.legend(title="Predicted Class")
plt.show()

# Visualize Linear Regression
plt.figure(figsize=(8, 6))
plt.scatter(X_test, y_test, c=predictions_linreg)
plt.title("Linear Regression")
plt.xlabel("Message Length")
plt.ylabel("Sender ID")
plt.legend(title="Predicted Value")
plt.show()

# Create Word Embeddings using Word2Vec
sentences = [str(sentence).split() for sentence in all_messages]
embeddings_model = Word2Vec(
    sentences=sentences, vector_size=100, window=5, min_count=1, workers=4
)

# Extract word embeddings
embeddings = embeddings_model.wv.vectors

# Perform dimensionality reduction using t-SNE
tsne = TSNE(n_components=2, random_state=42)
embeddings_tsne = tsne.fit_transform(embeddings)

# Visualize Word Embeddings
plt.figure(figsize=(8, 6))
plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1])
plt.title("Word Embeddings")
plt.xlabel("Dimension 1")
plt.ylabel("Dimension 2")
plt.legend(title="Two dimensions obtained after performing dimensionality reduction using t-SNE")
plt.show()

# Display results
print("Accuracy of Logistic Regression: ", accuracy_logreg)
print("Mean Squared Error of Linear Regression: ", mse_linreg)
print("Word Embeddings: ", embeddings_tsne)
